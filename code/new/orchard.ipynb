{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rVTmio2zmeL",
        "outputId": "725f962c-db06-4edf-ed6d-3e8a92220256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Action(Enum):\n",
        "  MOVE_UP = 0\n",
        "  MOVE_DOWN = 1\n",
        "  MOVE_LEFT = 2\n",
        "  MOVE_RIGHT = 3\n",
        "  STAY = 4\n",
        "\n",
        "class Index:\n",
        "  APPLE = 0\n",
        "  AGENT = 1"
      ],
      "metadata": {
        "id": "t34ORGkTTfZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import Env, spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class OrchardEnv(Env):\n",
        "  def __init__(self, grid_size=(5, 5), spawn_prob=0.2, num_agents=1,\n",
        "               max_apples_per_cell=100, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Constructor to create an orchard environment for multiple agents to\n",
        "    work with.\n",
        "    \"\"\"\n",
        "    # environment configuration\n",
        "    super(OrchardEnv, self).__init__()\n",
        "    self.grid_side = grid_size\n",
        "    self.spawn_prob = spawn_prob\n",
        "    self.num_agents = num_agents\n",
        "    self.max_apples_per_cell = max_apples_per_cell\n",
        "    self.current_step = 0\n",
        "    self.max_steps = max_steps\n",
        "    self.grid_size = grid_size\n",
        "\n",
        "    # action space: 4 directions and staying\n",
        "    self.action_space = spaces.Discrete(5)\n",
        "\n",
        "    # observation space: 3D tensor\n",
        "    # First 2 indices specify row and column.\n",
        "    # Last index is indexed using Index.APPLE or Index.AGENT.\n",
        "    self.observation_space = spaces.Box(\n",
        "      low=np.zeros((grid_size[0], grid_size[1], 2), dtype=np.float32),\n",
        "      high=np.stack([np.full((grid_size[0], grid_size[1]), max_apples_per_cell),\n",
        "                   np.full((grid_size[0], grid_size[1]), num_agents)], axis=-1),\n",
        "      shape=(grid_size[0], grid_size[1], 2),\n",
        "      dtype=np.float32\n",
        "    )\n",
        "\n",
        "    # Initialize grids to keep track of apple and agent states\n",
        "    # Agents are randomly placed in the orchard to begin with\n",
        "    self.apples = np.zeros((grid_size[0], grid_size[1]))\n",
        "    self.agent_positions = (np.random.choice(grid_size[0], num_agents),\n",
        "                        np.random.choice(grid_size[1], num_agents))\n",
        "\n",
        "  def _is_done(self):\n",
        "    \"\"\"\n",
        "    Returns whether we have completed max_steps many steps.\n",
        "    \"\"\"\n",
        "    return self.current_step >= self.max_steps\n",
        "\n",
        "  def _get_observations(self):\n",
        "    \"\"\"\n",
        "    Returns the observation generated from the orchard with dimensions\n",
        "    grid_size[0] X grid_size[1] X 2.\n",
        "    \"\"\"\n",
        "    # Initialize empty observation matrix.\n",
        "    observation = np.zeros((self.grid_size[0], self.grid_size[1], 2),\n",
        "                            dtype=np.float32)\n",
        "\n",
        "    # Populate observation of number of apples.\n",
        "    observation[:, :, Index.APPLE] = self.apples\n",
        "\n",
        "    # Populate observation of number of agents\n",
        "    for agent_row, agent_col in zip(*self.agent_positions):\n",
        "      observation[agent_row, agent_col, Index.AGENT] += 1\n",
        "\n",
        "    return observation\n",
        "\n",
        "  def reset(self):\n",
        "    # reset steps to 0.\n",
        "    self.current_step = 0\n",
        "\n",
        "    # reset the apple and agent positions\n",
        "    self.apples = np.zeros((self.grid_size[0], self.grid_size[1]))\n",
        "    self.agent_positions = (np.random.choice(self.grid_size[0], self.num_agents),\n",
        "                        np.random.choice(self.grid_size[1], self.num_agents))\n",
        "\n",
        "\n",
        "    observation = np.zeros((self.grid_size[0], self.grid_size[1], 2), dtype=np.float32)\n",
        "\n",
        "    # Populate observation of the number of apples.\n",
        "    observation[:, :, Index.APPLE] = self.apples\n",
        "\n",
        "    # Populate observation of the number of agents\n",
        "    for agent_row, agent_col in zip(*self.agent_positions):\n",
        "      observation[agent_row, agent_col, Index.AGENT] += 1\n",
        "\n",
        "    return observation\n",
        "\n",
        "  def _move_agent(self, agent_id, action):\n",
        "    \"\"\"\n",
        "    Update the position of an agent.\n",
        "    \"\"\"\n",
        "    current_row = self.agent_positions[0][agent_id]\n",
        "    current_col = self.agent_positions[1][agent_id]\n",
        "\n",
        "    if action == Action.MOVE_UP:\n",
        "      new_row = max(0, current_row - 1)\n",
        "      self.agent_positions[0][agent_id] = new_row\n",
        "    elif action == Action.MOVE_DOWN:\n",
        "      new_row = min(self.grid_size[0] - 1, current_row + 1)\n",
        "      self.agent_positions[0][agent_id] = new_row\n",
        "    elif action == Action.MOVE_LEFT:\n",
        "      new_col = max(0, current_col - 1)\n",
        "      self.agent_positions[1][agent_id] = new_col\n",
        "    elif action == Action.MOVE_RIGHT:\n",
        "      new_col = min(self.grid_size[1] - 1, current_col + 1)\n",
        "      self.agent_positions[1][agent_id] = new_col\n",
        "\n",
        "  def _move_agents(self, actions):\n",
        "    \"\"\"\n",
        "    Update the positions of all agents.\n",
        "    \"\"\"\n",
        "    for agent_id, action in enumerate(actions):\n",
        "      self._move_agent(agent_id, action)\n",
        "\n",
        "  def _pickup_apples(self):\n",
        "      \"\"\"\n",
        "      Pick up apples in each cell based on the number of agents\n",
        "      and apples present.\n",
        "\n",
        "      Returns a set of agents that have picked up an apple.\n",
        "      \"\"\"\n",
        "      # Group agents by their locations\n",
        "      agent_locations = defaultdict(list)\n",
        "      for agent_id, (agent_row, agent_col) in enumerate(zip(*self.agent_positions)):\n",
        "        agent_locations[(agent_row, agent_col)].append(agent_id)\n",
        "\n",
        "\n",
        "      # Distribute apples in each cell.\n",
        "      rewarded_agents = []\n",
        "      for (row, col), agents_in_cell in agent_locations.items():\n",
        "        num_apples_in_cell = int(self.apples[row, col])\n",
        "        num_agents_in_cell = len(agents_in_cell)\n",
        "\n",
        "        if num_agents_in_cell > num_apples_in_cell:\n",
        "          self.apples[row, col] = 0\n",
        "          agents_to_pick_apples = random.sample(agents_in_cell, num_apples_in_cell)\n",
        "          rewarded_agents.extend(agents_to_pick_apples)\n",
        "        else:\n",
        "          self.apples[row, col] -= num_agents_in_cell\n",
        "          rewarded_agents.extend(agents_in_cell)\n",
        "\n",
        "      return set(rewarded_agents)\n",
        "\n",
        "  def _spawn_apples(self):\n",
        "    \"\"\"\n",
        "    Spawn apples in the orchard based on the spawn probability.\n",
        "    \"\"\"\n",
        "    for row in range(self.grid_size[0]):\n",
        "      for col in range(self.grid_size[1]):\n",
        "        new_apple_can_spawn = random.random() < self.spawn_prob\n",
        "        num_apples_below_max = self.apples[row, col] < self.max_apples_per_cell\n",
        "        if new_apple_can_spawn and num_apples_below_max:\n",
        "          self.apples[row, col] += 1\n",
        "\n",
        "  def step(self, actions):\n",
        "    \"\"\"\n",
        "    Performs 1 step in the orchard environment.\n",
        "    \"\"\"\n",
        "    self._move_agents(actions)\n",
        "    rewarded_agents = self._pickup_apples()\n",
        "    self._spawn_apples()\n",
        "\n",
        "    self.current_step += 1\n",
        "\n",
        "    global_reward = len(rewarded_agents)\n",
        "    done = self._is_done()\n",
        "    observations = self._get_observations()\n",
        "    info = {\n",
        "      \"rewarded agents\": rewarded_agents\n",
        "    }\n",
        "\n",
        "\n",
        "    return observations, global_reward, done, info\n",
        "\n",
        "  def render(self):\n",
        "    for row in range(self.grid_size[0]):\n",
        "      for col in range(self.grid_size[1]):\n",
        "        cell_info = f\"[apples: {int(self.apples[row, col])}, agents: {int(np.sum(self.agent_positions[0] == row) & (self.agent_positions[1] == col))}]\"\n",
        "        print(cell_info, end=\" \")\n",
        "      print()"
      ],
      "metadata": {
        "id": "x5pOZkxn0s0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_orchard_env = OrchardEnv()\n",
        "test_orchard_env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6GQJi3Qd1c6",
        "outputId": "cfa1a06c-af08-4bf6-8261-346dc9fa3647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 1] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, _ = test_orchard_env.step([Action.MOVE_LEFT])\n",
        "test_orchard_env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XLJHNcBd9HD",
        "outputId": "27757ce8-c002-42b2-c21b-58d3fb382894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 1, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 0, agents: 1] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 1, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 1, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 1, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, _ = test_orchard_env.step([Action.MOVE_DOWN])\n",
        "test_orchard_env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZj7_d7reIne",
        "outputId": "fb03d0f4-7b84-4dac-839b-a698b104c185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 1, agents: 0] [apples: 0, agents: 0] [apples: 1, agents: 0] \n",
            "[apples: 0, agents: 0] [apples: 1, agents: 0] [apples: 1, agents: 0] [apples: 1, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 1, agents: 0] [apples: 0, agents: 1] [apples: 0, agents: 0] [apples: 1, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 1, agents: 0] [apples: 2, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n",
            "[apples: 1, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] [apples: 0, agents: 0] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done, info = test_orchard_env.step([Action.STAY])\n",
        "\n",
        "print(reward)\n",
        "print(info[\"rewarded agents\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaBjTaVQeOc9",
        "outputId": "242ed1fa-df15-4029-c420-0ce786c24d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "HKnZsc-JXQQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "  def __init__(self, observation_space, action_space, learning_rate=0.001, gamma=0.99):\n",
        "    self.observation_space = observation_space\n",
        "    self.action_space = action_space\n",
        "    self.learning_rate = learning_rate\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # Q-network\n",
        "    input_size = np.prod(observation_space.shape)\n",
        "    output_size = action_space.n\n",
        "    self.q_network = QNetwork(input_size, output_size)\n",
        "    self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Epsilon-greedy exploration\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.epsilon_min = 0.01\n",
        "\n",
        "  def select_action(self, observation):\n",
        "    if np.random.rand() < self.epsilon:\n",
        "      return np.random.choice(self.action_space.n)\n",
        "    else:\n",
        "      q_values = self.q_network(torch.tensor(observation.flatten(), dtype=torch.float32))\n",
        "      return torch.argmax(q_values).item()\n",
        "\n",
        "  def update_q_function(self, state, action, reward, next_state, done):\n",
        "    state = torch.tensor(state.flatten(), dtype=torch.float32)\n",
        "    next_state = torch.tensor(next_state.flatten(), dtype=torch.float32)\n",
        "\n",
        "    q_values = self.q_network(state)\n",
        "    next_q_values = self.q_network(next_state)\n",
        "\n",
        "    q_value = q_values[action]\n",
        "    next_q_value = torch.max(next_q_values).item() if not done else 0\n",
        "\n",
        "    target = torch.tensor(reward + self.gamma * next_q_value, dtype=torch.float32)\n",
        "    # print(type(q_value))\n",
        "    # print(type(target))\n",
        "    # return\n",
        "    loss = nn.MSELoss()(q_value, target)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # Decay epsilon for exploration\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
      ],
      "metadata": {
        "id": "1qZaaCnNYap9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = OrchardEnv()\n",
        "agent = QLearningAgent(env.observation_space, env.action_space)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "  observation = env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  while True:\n",
        "    action = agent.select_action(observation)\n",
        "    observation, global_reward, done, info = env.step([action])\n",
        "    agent_reward = 0 in info[\"rewarded agents\"]\n",
        "    if global_reward > 0:\n",
        "      print(\"here\")\n",
        "    agent.update_q_function(observation, action, agent_reward, observation, done)\n",
        "    break\n",
        "\n",
        "    total_reward += agent_reward\n",
        "\n",
        "    if done:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "G7vgFhTyYjOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test_episodes = 10\n",
        "test_total_rewards = []\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    observation = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.select_action(observation)\n",
        "        observation, _, done, info = env.step([action])\n",
        "        agent_reward = 0 in info[\"rewarded agents\"]\n",
        "\n",
        "        total_reward += agent_reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    test_total_rewards.append(total_reward)\n",
        "\n",
        "# Print average test performance\n",
        "average_test_reward = sum(test_total_rewards) / num_test_episodes\n",
        "print(f\"Average Test Reward: {average_test_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9mxYg6Ueo43",
        "outputId": "4307001d-af01-4af9-b503-ac0330d34115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Reward: 200.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GreedyApplePickerAgent:\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # Extract relevant information from the observation\n",
        "        apples = observation[:, :, 0]  # Assuming the first channel represents the number of apples\n",
        "\n",
        "        # Get the coordinates of the agent\n",
        "        agent_row, agent_col = np.where(observation[:, :, 1] > 0)\n",
        "\n",
        "        if len(agent_row) == 0:\n",
        "            # No agent found in the observation\n",
        "            return 0  # Move Up as a default action\n",
        "\n",
        "        agent_row, agent_col = agent_row[0], agent_col[0]\n",
        "\n",
        "        # Find the coordinates of all apples in the observation\n",
        "        apple_rows, apple_cols = np.where(apples > 0)\n",
        "\n",
        "        if len(apple_rows) == 0:\n",
        "            # No apples found, stay in the same spot\n",
        "            return 0  # Stay in the same spot\n",
        "\n",
        "        # Calculate distances to all apples\n",
        "        distances = np.abs(apple_rows - agent_row) + np.abs(apple_cols - agent_col)\n",
        "\n",
        "        # Find the index of the closest apple\n",
        "        closest_apple_idx = np.argmin(distances)\n",
        "\n",
        "        # Calculate the direction towards the closest apple\n",
        "        move_up = apple_rows[closest_apple_idx] < agent_row\n",
        "        move_down = apple_rows[closest_apple_idx] > agent_row\n",
        "        move_left = apple_cols[closest_apple_idx] < agent_col\n",
        "        move_right = apple_cols[closest_apple_idx] > agent_col\n",
        "\n",
        "        # Choose the action based on the direction towards the closest apple\n",
        "        if move_up:\n",
        "            return 0  # Move Up\n",
        "        elif move_down:\n",
        "            return 1  # Move Down\n",
        "        elif move_left:\n",
        "            return 2  # Move Left\n",
        "        elif move_right:\n",
        "            return 3  # Move Right\n",
        "        else:\n",
        "            return 4  # Stay in the same spot\n"
      ],
      "metadata": {
        "id": "BhbHeftfe5u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = OrchardEnv()\n",
        "agent = GreedyApplePickerAgent(env.observation_space, env.action_space)\n",
        "\n",
        "# Testing loop with the GreedyApplePickerAgent\n",
        "num_test_episodes = 10\n",
        "test_total_rewards = []\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    observation = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.select_action(observation)\n",
        "        observation, _, done, info = env.step([action])\n",
        "        agent_reward = 0 in info[\"rewarded agents\"]\n",
        "\n",
        "        total_reward += agent_reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    test_total_rewards.append(total_reward)\n",
        "\n",
        "# Print average test performance\n",
        "average_test_reward = sum(test_total_rewards) / num_test_episodes\n",
        "print(f\"Average Test Reward (Greedy Apple Picker Agent): {average_test_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ2-2Tb7e7P7",
        "outputId": "a7ef834d-74da-47e7-b472-18af3f2193ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Reward (Greedy Apple Picker Agent): 201.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = OrchardEnv(num_agents=5)  # Initialize environment with 5 agents\n",
        "agents = [QLearningAgent(env.observation_space, env.action_space) for _ in range(env.num_agents)]  # Initialize agents\n",
        "test_total_rewards = []\n",
        "\n",
        "num_test_episodes = 10\n",
        "for episode in range(num_test_episodes):\n",
        "\n",
        "    observations = env.reset()  # Reset environment and get initial observations\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        actions = [agent.select_action(observation) for agent, observation in zip(agents, observations)]  # Get action from each agent\n",
        "\n",
        "        next_observations, rewards, done, info = env.step(actions)  # Environment step based on actions\n",
        "\n",
        "        for i, agent in enumerate(agents):\n",
        "            total_reward += 1 if i in info[\"rewarded agents\"] else 0\n",
        "\n",
        "        observations = next_observations  # Update observations for next step\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    test_total_rewards.append(total_reward / env.num_agents)\n",
        "\n",
        "\n",
        "average_test_reward = sum(test_total_rewards) / num_test_episodes\n",
        "print(f\"Average Test Reward (5 QLearningAgents): {average_test_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39OlHO4Z6EyZ",
        "outputId": "b0aec7b7-9833-45c0-fbc0-27b0f395bd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Reward (5 QLearningAgents): 194.45999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = OrchardEnv(num_agents=5)  # Initialize environment with 5 agents\n",
        "agents = [GreedyApplePickerAgent(env.observation_space, env.action_space) for _ in range(env.num_agents)]  # Initialize agents\n",
        "test_total_rewards = []\n",
        "\n",
        "num_test_episodes = 10\n",
        "for episode in range(num_test_episodes):\n",
        "\n",
        "    observations = env.reset()  # Reset environment and get initial observations\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        actions = [agent.select_action(observation) for agent in agents]  # Get action from each agent\n",
        "\n",
        "        observations, rewards, done, info = env.step(actions)  # Environment step based on actions\n",
        "\n",
        "        for i, agent in enumerate(agents):\n",
        "            total_reward += 1 if i in info[\"rewarded agents\"] else 0\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    test_total_rewards.append(total_reward / env.num_agents)\n",
        "\n",
        "\n",
        "average_test_reward = sum(test_total_rewards) / num_test_episodes\n",
        "print(f\"Average Test Reward (5 QLearningAgents): {average_test_reward}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "Tzjsqp5V_w83",
        "outputId": "64e65a07-251d-4abc-b9a9-20567be4740e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OrchardEnv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a0158e662956>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrchardEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize environment with 5 agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mGreedyApplePickerAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Initialize agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_total_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_test_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OrchardEnv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Work so Far\n",
        "1. Created an environment that is built with cleaner code.\n",
        "   - Adheres to interfaces that are quite popular in the RL communities.\n",
        "   - Uses `numpy` where applicable. Maybe, I can look into more code to parallelize.\n",
        "2. Created and tested a single Q-learning agent.\n",
        "   - Had lots of trouble understanding how to build the Q-network and train the agent.\n",
        "   - I will probably revisit this again next week.\n",
        "3. Created and tested a single greedy agent (moves towards nearest apple).\n",
        "   - This serves as the \"top line\" for what a single agent should be capable of doing.\n",
        "\n",
        "\n",
        "Intuitively speaking, the greedy agent should be optimal in solving this problem on an independent basis. Tests show that a greedy agent collects about 204.4 apples (on average) in 1000 steps.\n",
        "\n",
        "Given the Q-learning agent collects 194.9 apples (on average) in 1000 steps, these are pretty good results and show that the Q-learning setup should be correct.\n",
        "\n",
        "As next steps, I would like to better understand some of the code that I've written for Q-learning and also extend these tests to a multi-agent environment."
      ],
      "metadata": {
        "id": "188xm7QqfBcs"
      }
    }
  ]
}